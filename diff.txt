diff --git a/tensorrt_llm/_torch/attention_backend/flashinfer.py b/tensorrt_llm/_torch/attention_backend/flashinfer.py
index 4769c949..a0be9bd6 100644
--- a/tensorrt_llm/_torch/attention_backend/flashinfer.py
+++ b/tensorrt_llm/_torch/attention_backend/flashinfer.py
@@ -206,6 +206,11 @@ class FlashInferAttentionMetadata(AttentionMetadata):
         assert self.request_ids is not None
         block_ids_per_seq = self.kv_cache_manager.get_batch_cache_indices(
             self.request_ids)
+        paged_kv_indices = torch.tensor(
+            [x for block_ids in block_ids_per_seq for x in block_ids],
+            dtype=torch.int32)
+        self._paged_kv_indices[:paged_kv_indices.size(0)].copy_(
+            paged_kv_indices, non_blocking=True)
 
         # number of tokens in the kv cache for each sequence in the batch
         cached_token_lens = torch.tensor(
@@ -236,26 +241,13 @@ class FlashInferAttentionMetadata(AttentionMetadata):
                                              1])
 
         # number of cache blocks used by each sequence in the cache
-        # NOTE: do not use len(block_ids) - that will give you a number
-        # that can be too big if using chunked prefill/kv cache reuse
-        # since we allocate all blocks ahead of time.
-        num_blocks = ((kv_lens + self.page_size - 1) // self.page_size)
-        self.num_blocks = num_blocks.tolist()
+        self.num_blocks = [len(block_ids) for block_ids in block_ids_per_seq]
         self.num_context_blocks = sum(self.num_blocks[:self.num_contexts])
         self.num_generation_blocks = sum(self.num_blocks[self.num_contexts:])
 
-        paged_kv_indices_list = []
-        for i, block_ids in enumerate(block_ids_per_seq):
-            paged_kv_indices_list.extend(block_ids[:self.num_blocks[i]])
-
-        paged_kv_indices = torch.tensor(paged_kv_indices_list,
-                                        dtype=torch.int32)
-
-        self._paged_kv_indices[:paged_kv_indices.size(0)].copy_(
-            paged_kv_indices, non_blocking=True)
-
         # number of tokens in the last cache block used by each sequence
-        paged_kv_last_page_len = kv_lens - (num_blocks - 1) * self.page_size
+        paged_kv_last_page_len = kv_lens - (torch.Tensor(
+            self.num_blocks).int().cuda(non_blocking=True) - 1) * self.page_size
         self._paged_kv_last_page_len[:paged_kv_last_page_len.size(0)].copy_(
             paged_kv_last_page_len, non_blocking=True)
 
@@ -619,4 +611,4 @@ def _(
     attention_mask_type: int,
     attention_mask_data: Optional[torch.Tensor],
 ):
-    return torch.empty_like(q)
+    return torch.empty_like(q)
\ No newline at end of file
diff --git a/tensorrt_llm/_torch/pyexecutor/py_executor_creator.py b/tensorrt_llm/_torch/pyexecutor/py_executor_creator.py
index 89741e95..0cd1a95f 100644
--- a/tensorrt_llm/_torch/pyexecutor/py_executor_creator.py
+++ b/tensorrt_llm/_torch/pyexecutor/py_executor_creator.py
@@ -46,11 +46,11 @@ def create_py_executor(executor_config: ExecutorConfig,
             )
             executor_config.kv_cache_config.enable_block_reuse = False
 
-    if pytorch_backend_config.attn_backend == "FLASHINFER_STAR_ATTENTION" and executor_config.enable_chunked_context:
-        logger.warning(
-            f"Disabling chunked context for {pytorch_backend_config.attn_backend} backend"
-        )
-        executor_config.enable_chunked_context = False
+    # if pytorch_backend_config.attn_backend in ["FLASHINFER_STAR_ATTENTION", "FLASHINFER"] and executor_config.enable_chunked_context:
+    #     print(
+    #         f"Disabling chunked context for {pytorch_backend_config.attn_backend} backend"
+    #     )
+    #     executor_config.enable_chunked_context = False
 
     if executor_config.max_num_tokens is None:
         executor_config.max_num_tokens = 8192
